#JMM #Memory-Barriers 

#  不能让CPU闲着
- 场景，`CPU 0` 和`CPU 1` 同时拥有某个缓存行，两个缓存行都处于`Shared`状态；
- `CPU 0` 想对自己的缓存行执行 *write* 操作，根据 [[MESI 协议]] `CPU 0` 必须先发送 `Invalidate` 消息让`CPU 1`中的*缓存行失效*。

![[CPU等待stall示意.gif]]
- `CPU 0`必须等到`CPU 1`反馈了`Invalidate Acknowledge`之后才能确保自己可以操作缓存行；
- 从发出`Invalidate`直到收到`Invalidate Acknowledge`的这段时间，`CPU 0`一直处于**阻塞等待(stall)状态**。
- CPU 是宝贵的资源，不能闲置，硬件工程师为了解决这个问题，引入了 `Store Buffers`。

#  引入缓存 Store Buffer
![[加入store buffer后的计算机数据架构图.png]]
- 在CPU和Cache之间添加了一个**中间层——`Store Buffer`**。
- 当`CPU 0` 执行 *write* 指令时：
	1. 先把想要 *write* 的值写入到`Store Buffer`中；
	2. 再继续执行其他任务，无需等待`CPU 1`。
	3. `CPU 1` 返回之后，`CPU 0` 再将 `Store Buffer` 中的最新值写入到缓存行中。
- `Store Buffers`的引入解决了CPU闲置的问题，又引出了3个新问题。


## buffer和cache数据不同步问题

![[storebuffer 引入的不一致问题.png]]

- 上图左侧的代码中，`a` 和 `b` 的初始值为0，在大多数时候，最后的断言会为 True。
- 在如下场景中可能会不符合预期（断言为False）。
	1. 假设变量 `a` 的缓存行已经存在于 `CPU 1` 的 Cache 中；
	2. 变量 `b` 的缓存行已经存在于 `CPU 0` 的 Cache中。
	3. 引入 `Store Buffers` 后，*在 `CPU 0` 加载并执行上述代码*（注意：现代计算机中，*线程作为CPU任务调度的基本单位*，**不会被拆分到多个CPU中执行**，所以这里的代码只会被某一个CPU执行）：

![[引入storebuffer后CPU操作过程示意.png]]

1.  CPU 0 执行语句 `a = 1`;
2.  CPU 0 首先从自己的 Cache 中查找 `a`，没有找到；
3.  CPU 0 发送 `Read Invalidate` 消息 **读取并独占(为了写入)** 含有 `a` 的缓存行。通知其他 CPU，“我要用，你们都销毁！”；
4.  CPU 0 在 `Store Buffer` 缓存对 `a` 的赋值，即 `a = 1`。 CPU 0 继续向下执行，无需等待其他 CPU 对 `Read Invalidate` 消息的响应；
5. 此时 CPU 1 可能收到了来自 CPU 0 的 `Read Invalidate` 消息，于是把自己包含 `a` 的缓存行返回给 CPU 0，并把缓存行状态设置为 `Invalid`；
6. 若 CPU 0 收到来自 CPU 1 的缓存行，得到 a 值为 0，并存入缓存行中；
	1. 此时CPU 0 的缓存行中的 `a` 和 `b` 的状态都是`Exclusive`，因为这些缓存行都由 CPU 0 独占；
7.  CPU 0 准备向下执行语句 `b = a + 1`，此时会再次尝试读取 `a` 的值；
8. CPU 0 **从缓存中读取** `a`，此时值为 0；
9. CPU 0 取出之前在 `Store Buffer` 中存放的  `a = 1` 更新自己Cache中的 `a`，设置为1；
10. CPU 0 执行语句 `b = a + 1`时，不会重新从缓存中读取（无感知a已更新），直接使用第8步获取的`a`上`+ 1`，并更新缓存中的 `b`，设置缓存行的状态为 `Modified`；
11. CPU 0 执行断言操作，发现断言为 False。

- 这违反直觉，预期CPU就是完全按照代码的顺序执行的（至少最终结果应该表现地像CPU是完全按照代码的顺序执行的一样）。
- 原因是`CPU 0` 运行过程中出现了 `a` 的两份数据拷贝，一份是在`Store Buffer`中，一份是在 *Cache* 中，两份数据不同步
- 又引入了`Store Forwarding`来解决这个问题。

###  引入Store Forwarding

- CPU **数据加载操作直接使用 `Store Buffer`** ，而无需从Cache中获取，如下图所示。

![[引入storeforwarding后示意.png]]

- `Store Buffer`中的数据可以直接被CPU读取。
- 对应到上面的 `CPU 0` 的操作步骤，执行  `b = a + 1` 直接从`Store Buffer`中读取最新的 `a`，而不是从Cache中读取。

## CPU执行的乱序问题

- 下图左侧的代码，其中 `a` 和`b`的初始值为 0；
- 假设含有变量`a`的缓存行已经存在于`CPU 1`的Cache中，含有变量`b`的缓存行已经存在于`CPU 0`的Cache中。
- `CPU 0` 执行`foo`方法，`CPU 1` 执行`bar`方法。
- 正常情况下，`bar` 方法中的断言结果应该为True。

![[storebuffer引入的乱序问题示意.png]]

然而，按照下图中的执行顺序操作一遍之后，断言却是False！
![[storebuffer导致的乱序问题示意.png]]

1.  CPU 0 执行 `a = 1`，首先从 Cache 查找，没有找到；
2.  CPU 0 将`a`的新值`1`写入到自己的`Store Buffer`中；
3.  CPU 0 发送`Read Invalidate`消息并继续执行；
4.  CPU 1 执行 `while (b == 0) continue`，发现 `b` 不在自己的`Cache`中，于是发送 `Read` 消息并等待；
5.  CPU 0 执行 `b = 1`，由于`b`已经存在于自己的Cache中了，所以直接将 Cache 中的`b`修改为`1`，并修改包含`b`的缓存行的状态为`Modified`；
6.  CPU 0 收到 CPU 1 发出的`Read`消息，由于当前自己拥有的`b`是最新版本的，所以 CPU 0 把含有`b`的缓存行返回给CPU 1，同时修改自己的缓存行状态为`Shared`；
7.  CPU 1 收到来自CPU 0 的 `b` 数据（`b`为1），存入 Cache 中，并设置为`Shared`状态；
8.  CPU 1 结束 `while` 循环（`b` 为 `1`）；
9.  CPU 1 执行 `assert(a == 1)`，由于 Cache 中的 `a` 值是 `0`（假设此时还没收到来自 CPU 0 的`Read Invalidate`消息，因此 CPU 1 有理由认为缓存数据是合法的），因此 `assert(a==1)` 的结果为 False；
10.  CPU 1 终于收到来自CPU 0 的`Read Invalidate`消息了，按照约定把自己的`a`设置为`Invalid`状态，并且给CPU 0 发送`Invalidate Acknowledge`以及`Read Response`反馈；
11.  CPU 0 收到 CPU 1 的反馈，利用 `Store Buffer` 中的值更新 `a`。

![[storebuffer导致的乱序问题示意.gif]]


### 乱序原因
- `Store Buffer` 的加入导致 `Read Invalidate` 的发送是一个异步操作；
- 可能导致 CPU 1 接收到 CPU 0 的 `Read Invalidate` 消息太晚了，使得 `a=1;` 写操作在 Cache 中实际晚于 `b = 1`执行完；
- 从结果看，似乎**写操作被重排序了**，这就是**CPU的乱序执行**。
- 「乱序执行」是为了提高CPU的工作效率而诞生的：
	- 在大多数情况下并不会导致什么错误；
	- 在**多处理器（smp）并发**执行的时候可能会出现问题。
- 为了保证执行顺序，如果在 CPU 0 修改 `b` 之前，强制让 CPU 0 先完成前一个语句、对 `a` 的修改即可。
- CPU 提供了**内存屏障**（Memory Barrier,mb）指令解决这个问题。


## 内存屏障
- 修改一下 `foo` 方法，在 `b = 1` 之前添加一条内存屏障指令`smp_mb()`。

![[CPU内存屏障示意.png]]
### smp

> `smp` 的全称是 **Symmetrical Multi-Processing（对称多处理）技术**，是指在一个计算机上汇集了*一组处理器(指多 CPU )*，各 CPU 之间*共享内存子系统以及总线结构*的技术。

为什么要特意加上`smp`呢？
- 现代多处理器会乱序执行；
- 但在单个 CPU 上，指令通过**指令队列顺序获取并执行指令**，执行结果按照队列顺序返回寄存器；
- 也不存在多个 CPU 之间的通信、延迟等问题；
- 使得程序执行时所有的内存访问操作看起来像是按程序代码编写的顺序执行的，没必要使用内存屏障（不考虑编译器优化的情况）。

### 含义
- **内存屏障语句之后**，所有针对 Cache 的**写操作开始之前**，必须**先把 Store Buffer中 的数据全部写入到 Cache中**。
- 要保证 **存到Store Buffer中的数据有序地刷新到Cache中**，即可避免发生指令重排序了。

保证有序的实际做法
- 最简单的方式是让 CPU 等待，`CPU 0` 在执行第5步之前必须等着 `CPU 1` 给出反馈，直到清空自己的 `Store Buffer`，然后才能继续向下执行。效率低下。
- 或者让数据在 `Store Buffer` 中排队，先入先出，先刷新最先进入的，后边的必须等待。
- 此时本来可以直接写入Cache的操作（比如待操作的数据已经存在于自己的Cache中了）也必须先存到Store Buffer，然后依序进行刷新**。


### Store Buffers的容量问题

`Store Buffer`的容量通常很小，如果CPU此时需要对多个数据执行write操作，碰巧这些数据都不在该CPU的Cache中，那么该CPU只能发送对应的`Read Invalidate`指令了，同时新数据写入`Store Buffer`，非常容易导致`Store Buffer`空间被占满。

一旦`Store Buffer`被占满，CPU就只能**干等着**目标CPU完成`Read Invalidate`操作，并且返给自己`Invalidate Acknowledge`，当前CPU才能逐步将`Store Buffer`中的值刷新到Cache，腾出空间，然后继续执行。

CPU又又又闲下来了！所以我们肯定又得找个办法来解决这个问题。

出现这个问题的主要原因在于`Invalidate Acknowledge`的反馈速度太慢了！

因为CPU太老实了，它只有在确认自己的缓存行被设置为`Invalid`状态之后才会发送`Invalidate Acknowledge`。如果Cache的其他操作太频繁，“设置缓存行为Invalid状态”这个动作本身都会被延迟执行，更何况`Invalidate Acknowledge`的反馈动作呢，得等到猴年马月啊！

> 上面的GIF图中为了表现出「反馈慢」这种情况，我特意把`Invalidate`消息的发送速度设置地很慢，其实消息地发送速度非常快，只是CPU处理`Invalidate`消息的速度太慢了而已，望悉知。

如果不想等，想直接获取操作结果，你想到了什么？

没错，是异步！

实现方式就是再加一层消息队列——`Invalidate Queues`。

### 6.7. 引入Invalidate Queues

如下图，我们的硬件架构又升级了。在每个CPU的Cache之上，又设置了一个`Invalidate Queue`。

这样一来，收到`Invalidate`消息的CPU核心会把`Invalidate`消息直接存储到`Invalidate Queue`中，然后立即返回`Invalidate Acknowledge`，不需要再等着缓存行被实际设置成`Invalid`状态再发送，极大地提高了反馈速度。

你可能会问，万一`Invalidate Queues`中的`Invalidate`消息最终执行失败，但是`Acknowledge`消息已经返回了，这该怎么办呢？

好问题！答案是，我不知道。我们就当作硬件工程师绝对不会留下这个bug就是了。

![[引入invalid queue后架构.png]]

#### 6.7.1. Invalidate Queue引起的乱序问题

这个坑比较严重，很有可能直接干翻缓存屏障，再次引发乱序执行的问题。

老样子，还是先准备一下翻车的环境。如下图，我们假设变量`a`被`CPU 0` 和`CPU 1` 共享，为`Shared`状态；变量`b`被`CPU 0` 独占，为`Exclusive`状态；`CPU 0` 和`CPU 1` 分别执行`foo`和`bar`方法。

![[invalid queue 乱序状态示意.png]]

1.  CPU 0 执行`a = 1`，因为CPU 0 的Cache中已经有`a`了，状态为`Shared`，因此不能直接修改，需要发送`Invalidate`（不是`Read Invalidate`，因为自己有`a`）消息使其他缓存行失效；
2.  CPU 0 把试图修改的`a`的最新值`1`放入`Store Buffer`；
3.  CPU 0 发送`Invalidate`消息；
4.  CPU 1 执行`while(b == 0) continue;`发现`b`不在自己的Cache中，于是发送`Read`消息来获取`b`；
5.  CPU 1 收到来自CPU 0 的`Invalidate`消息，把该消息放入`Invalidate Queue`中（并没有立即让`a`失效），等候处理，然后**立刻**返回`Anknowledge`；
6.  CPU 0 收到`Acknowledge`消息，认为CPU 1 已经把`a`值设置为`Invalid`了，于是放心地把`Store Buffer`中的数据刷新到自己的Cache中，此时CPU 0 Cache中的`a`为`1`，状态为`Modified`；然后就可以直接越过`smp_mb()`内存屏障，因为现在`Store Buffer`中的数据已经空了，满足内存屏障的约束条件。
7.  CPU 0 执行`b = 1`，因为其独占了`b`，所以可以直接在Cache中修改`b`的值，此时`b`缓存行的状态为`Modified`；
8.  CPU 0 收到来自CPU 1 的`Read`消息，将修改之后的`b`缓存行返回，并修改自己Cache中的`b`缓存行的状态为`Shared`；
9.  CPU 1 收到包含`b`的缓存行数据，放在自己的Cache中，此时CPU 1 的Cache同时拥有了`a`和`b`；
10.  CPU 1 结束执行`while(b == 0) continue;`因为此时CPU 1 读到的`b`已经是`1`了；
11.  CPU 1 开始执行`assert(a == 1)`，CPU 1 从自己的Cache读到`a`为`0`，断言为False。
12.  CPU 1 开始处理`Invalidate Queue`队列，令Cache中的`a`失效，但是为时已晚！

![[invalid queue执行过程.gif]]


问题很明显出在第11步，这就是臭名昭著著名的**可见性问题**。`CPU 0` 修改了`a`的值，`CPU 1` 却不知道或者说知道的太晚！如果在第11步读取`a`的值之前就赶紧刷新`Invalidate Queue`中的消息，让`a`失效就好了，这样`CPU 1` 就不得不重新`Read`，得到的结果自然就是`1`了。

原因搞明白了，怎么解决呢？内存屏障再一次闪亮登场！

#### 6.7.2. 内存屏障的另一个功能

上文已经解释了内存屏障的功能，再抄一遍加深印象：

**1.在内存屏障语句之后的所有针对Cache的**`写操作`**开始之前，必须先把`Store Buffer`中的数据全部刷新到Cache中。**

其实内存屏障还有另一个功能：

**2.在内存屏障语句之后的所有针对Cache的**`读操作`**开始之前，必须先把`Invalidate Queue`中的数据全部作用到Cache中。**

使用缓存屏障之后的代码就变成了这个样子：

![[内存屏障刷新invalid queue.png]]

`bar`方法在`assert`之前添加了内存屏障，意味着在获取`a`的值之前，所有在`Invalidate Queue`中的`Invalidate`消息必须作用到Cache中。

至此，我们再次用内存屏障解决了可见性问题。

问题还没有结束......

## 7. 读内存屏障 & 写内存屏障

内存屏障有两个功能，在`foo`方法中实际发挥作用的是功能1，功能2并没有派上用场；同理，在`bar`方法中实际发挥作用的是功能2，功能1并没有派上用场。于是很多不同型号的CPU架构（不是所有）将内存屏障功能分为了**读内存屏障**和**写内存屏障**，具体如下。

-   `smp_mb`（全内存屏障，包含读和写全部功能）
-   `smp_rmb`（read memory barrier，仅包含功能2）
-   `smp_wmb`（write memory barrier，仅包含功能1）

上文已经解释地挺清楚了，因此就不再重复介绍`smp_rmb`和`smp_wmb`的作用了。直接看修改之后的代码吧。

![[引入读写屏障.png]]

## 8. 总结

计算机的演进就是一部反复挖坑、填坑的发展史。

为了解决内存和CPU之间速度差异过大的问题，引入了高速缓存Cache，结果导致了缓存一致性问题；

为了达到缓存一致的效果，CPU之间需要沟通啊，于是又设计了各种消息传递，结果消息传递导致了CPU的偶尔闲置；

为了不让CPU停下来，硬件工程师加入了写缓冲——`Store Buffer`，这一下子带来了3个问题！

第一个问题比较简单，通过引入`Store Forwarding`解决了；

第二个问题是操作重排序问题，我们又引入了内存屏障的第一个大招；

第三个问题是由于`Store Buffer`空间限制导致CPU又闲下来了，于是又设计了`Invalidate Queues`，然后又导致了乱序执行和可见性问题；

通过使用内存屏障的全部大招终于解决了乱序执行和可见性问题，又引出了大招伤害性过强的问题，于是又拆分成了更细粒度的`读屏障`和`写屏障`。。。。。。






# 参考
1. [说透缓存一致性与内存屏障](https://www.cnblogs.com/chanmufeng/p/16523365.html)