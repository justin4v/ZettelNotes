#IO-model #Problem-and-Solutions 

# 问题
1. 如何突破原始 I/O 的效率限制？ 
2. 如何提高IO的效率，应对大量客户端（多路复用），提高响应速度？

# 输入步骤
1. 等待*数据从网络中到达*。当所等待的分组到达的时候，它就被复制到内核中的某个缓冲区中；
2. 准备好后，把数据*从内核空间拷贝到应用（用户）空间*。

# 阻塞式 I/O(Blocking IO)
- 应用程序中进程发起 IO 调用后至内核执行 IO 操作返回结果之前，若发起调用的*线程一直处于等待状态*，则此次IO操作为阻塞IO。阻塞IO简称 *BIO*，*Blocking IO*。
- 其处理流程如下图所示
![[阻塞式IO模型.png]]
- 经典的应用：**阻塞 socket、Java BIO**。
- 缺点：
	1. 如果内核数据没准备好，用户进程将*一直阻塞*，**浪费性能**；
	2. 可用**非阻塞IO**优化

# 非阻塞式 I/O(Nonblocking IO)
- 如果内核数据还没准备好，可以先返回错误信息给用户进程，让它不需要等待；
- 进程通*过轮询的方式再来请求*。这就是*非阻塞 IO，NIO*，流程图如下
![[非阻塞IO模型.png]]

非阻塞IO的流程如下：
-   应用进程向操作系统内核，发起`recvfrom`读取数据。
-   操作系统内核数据没有准备好，立即返回`EWOULDBLOCK`错误码。
-   应用程序轮询调用，继续向操作系统内核发起`recvfrom`读取数据。
-   操作系统内核数据准备好了，从内核缓冲区拷贝到用户空间。
-   完成调用，返回成功提示。

- 简称**NIO**，`Non-Blocking IO`。
- 相对于阻塞IO，大幅提升了性能；
- 但是依然存在**性能问题**，即**频繁的轮询**，导致频繁的系统调用，同样会消耗大量的CPU资源。
- 可用**IO复用模型**解决。



# I/O 多路复用(IO multiplexing)

既然**NIO**无效的轮询会导致CPU资源消耗，我们等到内核数据准备好了，主动通知应用进程再去进行系统调用，那不就好了嘛？

## 文件描述符
- linux 系统中高效查找文件？
- linux 一切皆文件，用一个非负整数标记打开的文件；
- **文件描述符 fd**(File Descriptor)。

## IO复用思路
- 系统给提供**一类函数**（如**select、poll、epoll**：
	- 可以同时监控多个 `fd` 的操作；
	- 一旦任何一个内核数据就绪，应用进程就发起 `recvfrom` 调用准备接受数据。

### IO多路复用之select
- 应用进程调用select函数，同时监控多个`fd`；
- `select` 监控的`fd`中，只要有任何一个数据状态准备就绪了，`select`函数就会返回可读状态；
- 应用进程再发起`recvfrom`请求去读取数据。

![[IO多路复用-select.png]]


- 非阻塞IO模型（NIO）中，需要 `N` 次*轮询调用*;
- `select` IO多路复用模型，*只需发起一次系统调用*，大大优化了性能。

`select`缺点：
- 监听的 *IO 最大连接数有限*，在Linux系统上一般为1024。
- select 函数返回后，通过*遍历 `fdset`*，找到就绪的描述符`fd`。（仅知道有I/O事件发生，却不知是哪几个流）

### poll
- select **存在连接数限制**，又提出了**poll**。
- select相比，**poll** 解决了**连接数限制问题**。
- 但是 selec t和 poll 一样，需要*遍历文件描述符* 确定已经就绪的`socket`。
- 如果同时连接的大量客户端在一时刻只有极少处于就绪状态，伴随着监视的描述符数量的增长，**效率也会线性下降**。

### epoll

为了解决`select/poll`存在的问题，多路复用模型`epoll`诞生，它采用 *事件驱动* 来实现，流程图如下：

![[IO多路复用-epoll.png]]

- **epoll**先通过`epoll_ctl()`来注册一个`fd`（文件描述符）；
- 某个 `fd` 就绪时，内核会采用*回调机制*，迅速激活这个 `fd`；
- 进程调用 `epoll_wait()` 时便得到事件通知。
- 省去了 **遍历文件描述符**过程，而是用**监听事件回调**的的机制。

### select、poll、epoll对比
![[select-poll-epoll对比.png]]


- **epoll** 优化了 IO 的执行效率;
- 缺点：
	- 进程调用 `epoll_wait()` 时，仍然*会被阻塞*。
	- 优化：不用进程询问数据是否准备就绪，等发出请求后，一旦数据准备好了通知进程，就诞生了**信号驱动IO模型**。


# 信号驱动 I/O（SIGIO Signal Driven IO）
- 实际中*不常用*；
- 不再主动询问的方式去确认数据是否就绪，而是*向内核发送一个信号*（调用`sigaction`的时候建立一个`SIGIO`的信号）；
- 应用用户进程可以去做别的事，不用阻塞。
- 当内核数据准备好后，再通过 `SIGIO` 信号通知应用进程，数据准备好后的可读状态。
- 应用用户进程收到信号之后，立即调用`recvfrom`，去读取数据

![[信号驱动IO模型.png]]
- 应用进程发出信号后，立即返回，不会阻塞进程。
- 但是**数据复制到用户空间的过程是阻塞的**。不管是BIO，还是NIO，还是信号驱动，在数据从内核复制到应用缓冲的时候，都是阻塞的。

  
# 异步 I/O(Asynchronous IO)
- `AIO`实现了 *IO 全流程的非阻塞*；
- 应用进程发出系统调用后，立即返回；
- 但是立即返回的不是处理结果，而是*提交成功*的意思。
- 等内核数据准备好，将数据拷贝到用户进程缓冲区，*发送信号通知用户进程*IO操作执行完毕

![[异步IO模型.png]]
- 异步 IO 对其余四种同步 IO 的优化思路很简单，只需*向内核发送一次请求*，就可完成数据状态询问和数据拷贝的所有操作；
- 不用阻塞等待结果。
- 日常开发中，有类似的业务场景：
> 比如发起一笔批量转账，但是转账处理比较耗时，后端可以*先告知前端转账提交成功*，等到结果处理完，再通知前端结果即可。

## 对比
![[5种IO模型对比.png]]

![[IO模型分类.png]]

# 阻塞/非阻塞 IO
数据没有准备好时，IO 函数：
1. *直接返回（错误） ---- 非阻塞 IO*；
2. *等待数据就绪（调用线程处于阻塞 blocked 状态） ---- 阻塞 IO*。

## Blocking
被调用函数返回之前，调用线程会被挂起（线程进入阻塞状态，在这个状态下，cpu不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。

## Non-Blocking
在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。


# 同步/异步
## 同步/异步 IO
- synchronous I/O 导致调用线程阻塞；
- asynchronous I/O 不会导致调用线程阻塞；
- 所以[[IO模型]]中*前四种都是同步IO（第二步是阻塞）*，最后一个才是异步IO。

## 同步/异步调用
- 线程间关系；
- 同步调用指*调用方等待功能完成*，才从调用中返回并进行下一步操作。
- 异步调用指调用方发出功能调用后，*立即从功能调用中返回*（没有得到实际操作结果），并可进行下一步操作。
	- 如将调用交给 worker 处理后即可从调用中返回。worker 完成处理后，通过状态、通知和回调来通知原调用者处理结果。